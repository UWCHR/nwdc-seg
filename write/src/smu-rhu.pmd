```python, header, echo=False
# Author: University of Washington Center for Human Rights
# Title: GEO Group Internal Datasets on Use of Solitary Confinement at Northwest ICE Processing Center
# Date: 2020-11-12
# License: GPL 3.0 or greater
```

```python, footnote_functions, echo=False

# Functions for HTML formatted footnotes

fn_count = 1
fn_buffer = []

def fn(ref_text=str):
    global fn_count, fn_buffer
    ftn_sup = f'<a href="#_ftn{fn_count}" name="_ftnref{fn_count}"><sup>[{fn_count}]</sup></a>'
    ftn_ref = f'<a href="#_ftnref{fn_count}" name="_ftn{fn_count}"><sup>[{fn_count}]</sup></a> {ref_text}'
    fn_buffer.append(ftn_ref)
    fn_count = fn_count + 1
    print(ftn_sup)

def print_fn_refs():
    global fn_buffer
    for fn in fn_buffer:
        print(fn)
        print()

# Functions for labeling figures and tables

fig_count = 1
tab_count = 1

def fig_label():
    global fig_count
    print(f'Figure {fig_count}')
    fig_count = fig_count + 1

def tab_label():
    global tab_count
    print(f'Table {tab_count}')
    tab_count = tab_count + 1

```

# GEO Group Internal Datasets on Use of Solitary Confinement at Northwest ICE Processing Center
## UW Center for Human Rights

[Back to Data Appendix Index](index.html)


**Data analyzed:**

1. GEO Group Seregation Lieutenant's log of Restricted Housing Unit (**"RHU"**) placements at NWDC, released to UWCHR via FOIA litigation on August 12, 2020.
2. GEOTrack report of Segregation Mangement Unit (**"SMU"**) housing assignments at NWDC, released to UWCHR via FOIA litigation on August 12, 2020.

```python, imports, echo=True

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yaml

with open('input/cleanstats.yaml','r') as yamlfile:
    cur_yaml = yaml.load(yamlfile)
    smu_cleanstats = cur_yaml['output/smu.csv.gz']
    rhu_cleanstats = cur_yaml['output/rhu.csv.gz']

```

# GEOtrack report (SMU)

Original filename: `Sep_1_2013_to_March_31_2020_SMU_geotrack_report_Redacted.pdf`

Described by US DOJ attorneys for ICE as follows:

> “The GEOtrack report that was provided to Plaintiffs runs from September 1, 2013 to March 31, 2020. That report not only reports all placements into segregation, but it also tracks movement. This means that if an individual is placed into one particular unit then simply moves to a different unit, it is tracked in that report (if an individual is moved from H unit cell 101 to H unit cell 102, it would reflect the move as a new placement on the report).”

We refer to this dataset here by the shorthand "SMU" for "Special Management Unit".

The original file has been converted from PDF to CSV format using the [Xpdf pdftotext](https://www.xpdfreader.com/pdftotext-man.html) command line tool with `--table` option, and hand cleaned to correct OCR errors. The resulting CSV has been minimally cleaned in a private repository, dropping <%= smu_cleanstats['duplicates'] %> duplicated records and adding a unique identifier field, `hashid`; cleaning code availabe upon request.

The original file includes three redacted fields: `Alien #`, `Name`, and `Birthdate`. The file appears to be generated by a database report for the date range "9/1/2013 To 3/31/2020", presumably from the "GEOtrack" database referenced in the filename and by the DOJ attorneys for ICE. The original file has no unredacted unique field identifiers or individual identifiers.

```python, smu_import, echo=True

csv_opts = {'sep': '|',
            'quotechar': '"',
            'compression': 'gzip',
            'encoding': 'utf-8'}

smu = pd.read_csv('input/smu.csv.gz', **csv_opts)

assert len(set(smu['hashid'])) == len(smu)
assert sum(smu['hashid'].isnull()) == 0

data_cols = list(smu.columns)
data_cols.remove('hashid')

print(smu.info())

```

Here we display the first five records in the dataset (excluding `hashid` field):

<% print(smu[data_cols].head().to_html(border=0, index=False)) %>

```python, smu_date_convert, echo=True

# All dates convert successfully

assert pd.to_datetime(smu['assigned_dt'], errors='coerce').isnull().sum() == 0
smu['assigned_dt'] = pd.to_datetime(smu['assigned_dt'])
assert pd.to_datetime(smu['removed_dt'], errors='coerce').isnull().sum() == 0
smu['removed_dt'] = pd.to_datetime(smu['removed_dt'])

```
The GEOTrack database export timeframe conforms to `removed_dt` min/max values:

```python, smu_date_describe, echo=True

print(smu['assigned_dt'].describe())
print()
print(smu['removed_dt'].describe())


```

One record has a `removed_dt` value less than `assigned_dt`, but this is only a discrepancy in the hour values:

<% print(smu[data_cols].loc[smu['assigned_dt'] > smu['removed_dt']].to_html(border=0, index=False)) %>

<%= sum(smu['assigned_dt'] == smu['removed_dt']) %> records have a `removed_dt` value equal to `assigned_dt`, as seen in this sample of five records:

<% print(smu[data_cols].loc[smu['assigned_dt'] == smu['removed_dt']].head().to_html(border=0, index=False)) %>

We retain these records despite the logical inconsistency of these datetime fields, under the assumption that they represent short placements.

Recalculating segregation placement length based on date only results in same value as `days_in_seg` field.

Note that this calculation is not first day inclusive, as in the case of the RHU dataset and SRMS datasets. We will discard hourly data and calculate first day inclusive stay lengths elsewhere for comparison purposes, as no other dataset includes hourly placement or release times.

```python, smu_date_calc, echo=True

smu['days_calc'] = (pd.to_datetime(smu['removed_date']) - pd.to_datetime(smu['assigned_date'])) / np.timedelta64(1, 'D')
assert sum(smu['days_in_seg'] == smu['days_calc']) == len(smu)

```

The below desciptive statistics reflect first day exclusive stay lengths, including stays of 0 days. <%= sum(smu['days_calc'] < 1) %>, or <%= round((sum(smu['days_calc'] < 1) / len(smu) * 100), 2) %>% of records reflect stay lengths of less than one day, based on placement dates.

```python, smu_days_calc_describe, echo=True

print(smu['days_calc'].describe())

```

Annual median and mean placement lengths show an increase during calendar years 2017-2018:


```python, smu_med_avg_length, echo=True

g = smu.set_index('assigned_dt').groupby([pd.Grouper(freq='AS')])

smu_annual_med = g['days_calc'].median()

smu_annual_avg = g['days_calc'].mean()

print(smu_annual_med)
print()
print(smu_annual_avg)


```

Total placement counts per calendar year (note incomplete data for 2013, 2020):


```python, smu_total_placements, echo=True

smu_total_annual = smu.set_index('assigned_dt').groupby([pd.Grouper(freq='AS')])['hashid'].nunique()

print(smu_total_annual)

```

Stays over 14 days must be reported to ICE SRMS; here we flag long placements and calculate as a percent of total placements per year. Note that placements are by housing assignment in one of <%= len(smu['housing'].unique()) %> total housing locations, not cumulative stay length, so long stays may not be accurately represented here. The lack of unique identifiers makes it impossible to track cases of individuals in segregation for a total of 14 non-consecutive days during any 21 day period; or indiviudals with special vulnerabilities.

We find that long placements increase over time both absolutely and as proportion of total placements. However, this may simply reflect fewer transfers of individuals between housing assignments:

```python, smu_long_stays, echo=True

smu['long_stay'] = smu['days_calc'] > 14
long_stays_annual = smu.set_index('assigned_dt').groupby([pd.Grouper(freq='AS')])['long_stay'].sum()

print(long_stays_annual)
print()
print(long_stays_annual / smu_total_annual)


```

### Comparison with segregation placements reported by DHS inspectors

A [June 24-26, 2014 DHS inspection report](https://drive.google.com/file/d/1YDX4fOOJ3DCftWiQv7O_5jwA2eZ0ftWR/view?usp=sharing) for NWDC states, “Documentation reflects there were 776 assignments to segregation in the past year”. The DHS inspection report does not specify the source of the records cited.

The SMU dataset covers this period, albeit with only partial records for June-Sept 2013. The total count of placements recorded in the SMU dataset during this period, <%= len(smu.set_index('assigned_dt').loc['2013-06-01':'2014-06-30']) %> , is reasonably close to figure cited by DHS inspectors, which suggests an average of about <%= round((776 / 12), 0) %> placements per month:

```python, smu_dhs_compare, echo=True

### Monthly total placements during period of DHS inspection report:

dhs_period = smu.set_index('assigned_dt').loc[:'2014-06-30']

g = dhs_period.groupby(pd.Grouper(freq='M'))

print(g['hashid'].nunique())

dhs_period_complete = smu.set_index('assigned_dt').loc['2013-09-01':'2014-06-30']

g = dhs_period_complete.groupby(pd.Grouper(freq='M'))

dhs_period_complete_monthly_avg = g['hashid'].nunique().mean()


```

This is comparable to the average of <%= round(dhs_period_complete_monthly_avg, 1) %> placements per month reported in the SMU dataset during the period for which complete data exists (September 2013 - June 2014). 

# GEO Lieutenant's report (RHU)

Original file: `15_16_17_18_19_20_RHU_admission_Redacted.xlsx`

Log created and maintained by hand by GEO employee to track Restricted Housing Unit placements. Described by US DOJ attorneys for ICE as follows:

> “The spreadsheet runs from January 2015 to May 28, 2020 and was created by and for a lieutenant within the facility once he took over the segregation lieutenant duties. The spreadsheet is updated once a detainee departs segregation. The subjects who are included on this list, therefore, are those who were placed into segregation and have already been released from segregation. It does not include those individuals who are currently in segregation.”

We refer to this dataset here by the shorthand "RHU" for "Restricted Housing Unit".

The original file has been converted from XLSX to CSV format, with each annual tab saved as a separate CSV. The resulting CSVs have been concatenated and minimally cleaned in a private repository, dropping <%= rhu_cleanstats['duplicates'] %> duplicated records and adding a unique identifier field, `hashid`; cleaning code availabe upon request.

The original file includes two fully redacted fields: `Name` and `Alien #`; and one partially redacted field, `Placement reason`. The original file has no unredacted unique field identifiers or individual identifiers.


```python, rhu_import, echo=True

csv_opts = {'sep': '|',
            'quotechar': '"',
            'compression': 'gzip',
            'encoding': 'utf-8'}

rhu = pd.read_csv('input/rhu.csv.gz', **csv_opts)

assert len(set(rhu['hashid'])) == len(rhu)
assert sum(rhu['hashid'].isnull()) == 0

data_cols = list(rhu.columns)
data_cols.remove('hashid')

print(rhu.info())

```

Here we display the first five records in the dataset (excluding `hashid` field):

<% print(rhu[data_cols].head().to_html(border=0, index=False)) %>

## Dates and total days calculation

Inspection of the original Excel file shows that the `Total days` column values are often incorrecet, based on a missing cell formula. For example, on the "2020" spreadsheet tab, the `Total days` column values are integers which only occasionally align with calculated placement length based on the `Date in` and `Date out` columns. However, additional spreadsheet rows at the bottom of the sheet not containing values in other fields contain an Excel formula ("=(D138-C138)+1") which should have been used to calculate these values. Comparing calculated stay lengths with reported `Total days` suggests that this formula was not updated consistently, causing fields to become misaligned. Additionally, the "2015" spreadsheet tab includes many `Total days` values equal to "1", suggesting that the formula was applied incorrectly or with missing data.

We can recalculate actual stay lengths based on the formula cited above (inclusive of start days, with stays of less than one day calculated as "1"); or with the formula used for the "SMU" records above (exclusive of start days, with stays of less than one day calculated as "0"), for more consistent comparison with other datasets.

The above issue raises the possibility that other fields in addition to `Total days` may be misaligned in the original dataset. One fact mitigating this possibility is that no `Date out` values predate associated `Date in` values. We can also look more closely at qualitative fields to make an educated guess as to the data quality: for example, do `intial_placement` values suggesting disciplinary placements align with `placement_reason` values also consistent with disciplinary placements? However, we do not intend to use this dataset for detailed qualitative analysis; of most interest are total segregation placements and segregation stay lengths.

```python, rhu_date_setup, echo=True

rhu['date_in'] = pd.to_datetime(rhu['date_in'])
rhu['date_out'] = pd.to_datetime(rhu['date_out'])

# As noted above, no `date_out` values predate associated `date_in` values:

assert sum(rhu['date_in'] > rhu['date_out']) == 0

print(rhu['date_in'].describe())
print()
print(rhu['date_out'].describe())


```

Here we recalculate the total days field based on the first day inclusive formula in the original spreadsheet ("=(D138-C138)+1"):

```python, rhu_total_days_calc, echo=True

rhu['total_days_calc'] = (rhu['date_out'] - rhu['date_in']) / np.timedelta64(1, 'D') + 1

compare_pct = sum(rhu['total_days_calc'] == rhu['total_days']) / len(rhu) * 100

print(rhu['total_days'].describe())
print()
print(rhu['total_days_calc'].describe())

```

Only <% round(compare_pct, 2) %>% of original `total_days` values match their respective recalculated stay lengths in `total_days_calc`.

However, note that the above summary statistics for the original field (`total_days`) are very similar to the recalculated field (`total_days_calc`), suggesting that most values are present in the dataset but misaligned.

Therefore, we will conclude that it is correct to recalculate the `total_days` field here. We will follow the first day inclusive formula suggested in the original dataset here:

```python, rhu_recalculate_total_dats, echo=True

rhu['total_days'] = (rhu['date_out'] - rhu['date_in']) / np.timedelta64(1, 'D') + 1
rhu = rhu.drop('total_days_calc', axis=1)

print(rhu['total_days'].describe())

```

Annual median and mean placement lengths are relatively consistent, showing an apparent decrease during the first few months of 2020:


```python, rhu_med_avg_length, echo=True

g = rhu.set_index('date_in').groupby([pd.Grouper(freq='AS')])

rhu_annual_med = g['total_days'].median()

rhu_annual_avg = g['total_days'].mean()

print(rhu_annual_med)
print()
print(rhu_annual_avg)


```

Total placement counts per calendar year (note data for 2020 is incomplete):


```python, rhu_total_placements, echo=True

rhu_total_annual = rhu.set_index('date_in').groupby([pd.Grouper(freq='AS')])['hashid'].nunique()

print(rhu_total_annual)

```

Stays over 14 days must be reported to ICE SRMS; here we flag long placements and calculate as a percent of total placements per year. The lack of unique identifiers makes it impossible to track cases of individuals in segregation for a total of 14 non-consecutive days during any 21 day period. Inconsistencies and lack of information in `placement_reason` make it a poor candidate for flagging placements involving indiviudals with special vulnerabilities. We note an increasing proportion and absolute number of long placements during 2017-2019:

```python, rhu_long_stays, echo=True

rhu['long_stay'] = rhu['total_days'] > 14
long_stays_annual = rhu.set_index('date_in').groupby([pd.Grouper(freq='AS')])['long_stay'].sum()

print(long_stays_annual)
print()
print(long_stays_annual / rhu_total_annual)


```


<!---

---

## Notes

<%= print_fn_refs() %>

-->